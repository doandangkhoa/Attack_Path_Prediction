import pandas as pd
import numpy as np
import os
import json
import joblib
from collections import Counter

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold
from sklearn.metrics import (
    accuracy_score, f1_score, recall_score, precision_score, 
    confusion_matrix, classification_report, precision_recall_curve
)

# =========================
# CONFIGURATION
# =========================
DATA_PATH = "data/generated_attack_paths_policy_oracle.csv"
MODEL_DIR = "models"
MODEL_PATH = os.path.join(MODEL_DIR, "rf_baseline.pkl")
METRICS_PATH = os.path.join(MODEL_DIR, "metrics.json")

def train_baseline():
    print(f"üìÇ ƒêang t·∫£i d·ªØ li·ªáu t·ª´: {DATA_PATH} ...")

    if not os.path.exists(DATA_PATH):
        print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu. H√£y ch·∫°y data_generator.py tr∆∞·ªõc.")
        return

    df = pd.read_csv(DATA_PATH)

    # ==============================
    # 1. FEATURE SELECTION (ƒê√É S·ª¨A KH·ªöP V·ªöI EXTRACT_FEATURES)
    # ==============================
    trained_features = [
        'rank',

        # --- STRUCTURE ---
        'path_length',

        # --- WEIGHT (DIJKSTRA) ---
        'total_weight',
        'avg_weight',
        'min_weight',
        'std_weight',
        'deviation_weight',

        # --- DETECTION / NOISE ---
        'total_detection',
        'avg_detection',
        'max_detection',

        # --- ATTACK BEHAVIOR ---
        'exploit_count',
        'security_controls',
        'firewall_crossings',
        'privilege_gain',

        # --- CONTEXT ---
        'role_entropy',
        'role_score',
        'has_admin_access',
        'is_admin_source',
        'has_bastion',
        'has_mfa',
        # --- COMPOSITE RISK ---
        'risk_factor',
    ]

    # Ki·ªÉm tra c·ªôt thi·∫øu
    print("üßπ ƒêang ki·ªÉm tra d·ªØ li·ªáu...")
    missing_cols = [col for col in trained_features if col not in df.columns]
    if missing_cols:
        print(f"‚ö†Ô∏è C·∫£nh b√°o: C√°c c·ªôt sau b·ªã thi·∫øu trong CSV v√† s·∫Ω ƒë∆∞·ª£c ƒëi·ªÅn 0: {missing_cols}")
        for col in missing_cols:
            df[col] = 0

    # X·ª≠ l√Ω Infinity v√† NaN
    df = df.replace([np.inf, -np.inf], 0)
    df = df.fillna(0)

    X = df[trained_features]
    y = df["label"]

    print(f"üìä Ph√¢n ph·ªëi nh√£n: {sorted(Counter(y).items())}")

    # ==============================
    # 2. SPLIT DATA (Train/Val/Test)
    # ==============================
    # T√°ch Test (20%)
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # T√°ch Train (60%) v√† Val (20%)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
    )

    print(f"üìê Split sizes: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}")

    # ==============================
    # 3. RANDOM SEARCH OPTIMIZATION
    # ==============================
    print("üß† ƒêang t·ªëi ∆∞u h√≥a Random Forest...")

    rf = RandomForestClassifier(
        random_state=42,
        class_weight="balanced",
        n_jobs=-1
    )

    param_dist = {
        "n_estimators": [100, 200, 300],
        "max_depth": [None, 10, 20],
        "min_samples_split": [2, 5, 10],
        "min_samples_leaf": [1, 2, 4],
        "max_features": ["sqrt", "log2"]
    }

    search = RandomizedSearchCV(
        estimator=rf,
        param_distributions=param_dist,
        n_iter=20,
        scoring="f1",
        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),
        verbose=1,
        n_jobs=-1,
        random_state=42
    )

    search.fit(X_train, y_train)
    best_model = search.best_estimator_

    print(f"‚úÖ Tham s·ªë t·ªët nh·∫•t: {search.best_params_}")

    # ==============================
    # 4. THRESHOLD TUNING (TR√äN VAL SET)
    # ==============================
    print("\nüîç T√¨m ng∆∞·ª°ng t·ªëi ∆∞u tr√™n Validation set...")

    y_val_probs = best_model.predict_proba(X_val)[:, 1]
    precisions, recalls, thresholds = precision_recall_curve(y_val, y_val_probs)

    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
    best_idx = np.argmax(f1_scores)
    
    optimal_threshold = thresholds[best_idx]
    best_val_f1 = f1_scores[best_idx]

    print(f"   - Threshold m·∫∑c ƒë·ªãnh: 0.5000")
    print(f"   - Threshold t·ªëi ∆∞u  : {optimal_threshold:.4f}")
    print(f"   - Best Val F1       : {best_val_f1:.4f}")

    # ==============================
    # 5. FINAL EVALUATION (TR√äN TEST SET)
    # ==============================
    print("\nüß™ ƒê√°nh gi√° tr√™n t·∫≠p TEST...")

    y_test_probs = best_model.predict_proba(X_test)[:, 1]
    y_test_pred = (y_test_probs >= optimal_threshold).astype(int)

    acc = accuracy_score(y_test, y_test_pred)
    f1 = f1_score(y_test, y_test_pred)
    recall = recall_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    cm = confusion_matrix(y_test, y_test_pred)

    print("\n" + "="*50)
    print("    K·∫æT QU·∫¢ ƒê√ÅNH GI√Å (FINAL REPORT)    ")
    print("="*50)
    print(f"‚úÖ Accuracy  : {acc:.4f}")
    print(f"‚úÖ F1-Score  : {f1:.4f}")
    print(f"üéØ Recall    : {recall:.4f}")
    print(f"üéØ Precision : {precision:.4f}")
    print("\nConfusion Matrix:")
    print(cm)
    print("\nClassification Report:")
    print(classification_report(y_test, y_test_pred))

    # ==============================
    # 6. FEATURE IMPORTANCE & SAVE
    # ==============================
    print("\n‚≠ê Feature Importance:")
    feat_importance = {}
    importances = best_model.feature_importances_
    indices = np.argsort(importances)[::-1]

    for i in indices:
        name = trained_features[i]
        score = importances[i]
        feat_importance[name] = float(score)
        print(f"{name:20s} : {score:.4f}")

    os.makedirs(MODEL_DIR, exist_ok=True)
    joblib.dump(best_model, MODEL_PATH)

    metrics = {
        "accuracy": float(acc),
        "f1": float(f1),
        "recall": float(recall),
        "precision": float(precision),
        "optimal_threshold": float(optimal_threshold),
        "confusion_matrix": cm.tolist(),
        "feature_importance": feat_importance,
        "feature_names": trained_features # L∆∞u l·∫°i ƒë·ªÉ predict d√πng
    }

    with open(METRICS_PATH, "w") as f:
        json.dump(metrics, f, indent=4)

    print(f"\nüíæ Saved model to: {MODEL_PATH}")

if __name__ == "__main__":
    train_baseline()